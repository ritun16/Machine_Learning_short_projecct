{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "language_model.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7PIPIBT9Who",
        "colab_type": "text"
      },
      "source": [
        "# Language Model (Natural Language Proessing)\n",
        "## Overview\n",
        "\n",
        "\n",
        "*   Language Models are very important component in the field of Natural Language Processing.\n",
        "*   Language models are are very useful for the AI powered NLP applications.\n",
        "\n",
        "\n",
        "### *  Statistical Language Model\n",
        "\n",
        "\n",
        "*   These models use traditional statistical methods like **n-gram**, **hidden markov model**, **rule based models** etc.\n",
        "\n",
        "\n",
        "### *   Neural Language Model\n",
        "\n",
        "\n",
        "*   Due to the advancement of neural networks the language models are very powerful and used extensively.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Here we will discuss the **n-gram** and **lstm** based language models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfT4_oAXQ2LN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mounting my G-Drive to this session\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98DQWyExRB6a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dataset directory\n",
        "data_dir = '/content/drive/My Drive/dataset/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TleF7dRR_OLd",
        "colab_type": "text"
      },
      "source": [
        "## Statistical (tri-gram) language model\n",
        " * We will use nltk's reuters corpus for this excersise.\n",
        " * In case of trigram the first two tokens will be the key of the dictionary and the last token will be counted everytime and will be stored.\n",
        " * Once the whole corpus is read and stored, we will calculate the percentage of the last token for each first two tokens, which will give the prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1XD6MIYHD5t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import dependencies\n",
        "from nltk import trigrams\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import reuters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ki7JIKjyOT-m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download nltk datasets\n",
        "import nltk\n",
        "nltk.download('all')\n",
        "# nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVCl8yzf3PLC",
        "colab_type": "text"
      },
      "source": [
        "## Model Creation and prediction\n",
        "* Here we are trying to create a model based on python's nested dictionary structure where the key of the outer dictionary will be the two previous tokens (words) and the value will be another dictionary in which the key will be the third token (word) and its count.\n",
        "* Once the above model (nested dictionary) is created, we will calculate the percentage count of each third tokens with respect to the previous two tokens.\n",
        "It will help us to predict the output properly with a probability measure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEcA8xDzHJCp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model():\n",
        "  # Create a model based on a dictionary\n",
        "  model = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "  \n",
        "  for sentence in reuters.sents():\n",
        "    for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
        "      model[(w1, w2)][w3] += 1\n",
        "  return model\n",
        "\n",
        "\n",
        "def get_total_count(model):\n",
        "  # get the percentage value\n",
        "  total_count = dict()\n",
        "  for w1_w2, w3 in model.items():\n",
        "    key_count = sum(w3.values())\n",
        "    for w3_ in w3.keys():\n",
        "      model[w1_w2][w3_] /= key_count\n",
        "      model[w1_w2][w3_] *= 100\n",
        "  return model\n",
        "\n",
        "\n",
        "def predict_next(w1, w2, model):\n",
        "  next_words = tuple(model[w1, w2].items())\n",
        "  next_words = sorted(next_words,key=lambda x:x[-1], reverse=True)\n",
        "  next_words = list(map(lambda x:(x[0],round(x[-1], 2)), next_words))\n",
        "  next_words = next_words[0:3]\n",
        "  for word, perc in next_words:\n",
        "    print(word, perc, '%')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esgPLbrEaRPx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = create_model()\n",
        "model = get_total_count(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcO7VzpMUxHr",
        "colab_type": "code",
        "outputId": "f2be4188-ed92-4bb8-d89e-509b016df336",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "predict_next('the', 'time', model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "of 38.18 %\n",
            "being 18.18 %\n",
            ", 4.55 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jI8KGdmW3oN_",
        "colab_type": "text"
      },
      "source": [
        "## Observations\n",
        "* As we can see that the model works preety fine like an auto completion tool which works similarly like a mobile device.\n",
        "* But there are some limitations to this approach.\n",
        "* ### Limitations\n",
        "\n",
        "\n",
        "* Typically if we increase the 'n' the model will try to perform better, but eventually we will need more computation power and huge resource of memory (RAM) for the same.\n",
        "*   Here in this approach we are building the model based on the probability of words co-occurring. It will give zero probability to all the words that are not present in the training corpus, which is not at all desirable.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKt2PnMBGes0",
        "colab_type": "text"
      },
      "source": [
        "## Neural Language Model (LSTM)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEC_sZ8538dE",
        "colab_type": "code",
        "outputId": "27a2379a-d8b5-41e6-c2f9-4c671f085277",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "file = data_dir + 'train_dataset_modified.txt'\n",
        "with open(file, 'r') as f:\n",
        "  data = f.read()\n",
        "print(data[0:100])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "When did Beyonce start becoming popular? in the late 1990s What areas did Beyonce compete in when sh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OKLPeBIR2YK",
        "colab_type": "code",
        "outputId": "75c6baec-fb05-41fa-be43-bc23816371a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9673809"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z93MNCeH4DgD",
        "colab_type": "code",
        "outputId": "70748784-685e-4544-e0e7-1eb0688cd32a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, GRU, Embedding\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qA41gLamxcfo",
        "colab_type": "text"
      },
      "source": [
        "## Clean the data\n",
        "* As the text has many unnecessary characters, punctuation, numbers, special characters etc. We need to clean them up for our model to predict only necessary things."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f16kLQLe4XRg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "def text_cleaner(text):\n",
        "  newString = text.lower()\n",
        "  newString = re.sub(r\"'s\\b\",\"\",newString)\n",
        "  newString = re.sub(\"[^a-zA-Z]\", \" \", newString)\n",
        "  newString = re.sub(r\"won't\", \"will not\", newString)\n",
        "  newString = re.sub(r\"can\\'t\", \"can not\", newString)\n",
        "\n",
        "  # general\n",
        "  newString = re.sub(r\"n\\'t\", \" not\", newString)\n",
        "  newString = re.sub(r\"\\'re\", \" are\", newString)\n",
        "  newString = re.sub(r\"\\'s\", \" is\", newString)\n",
        "  newString = re.sub(r\"\\'d\", \" would\", newString)\n",
        "  newString = re.sub(r\"\\'ll\", \" will\", newString)\n",
        "  newString = re.sub(r\"\\'t\", \" not\", newString)\n",
        "  newString = re.sub(r\"\\'ve\", \" have\", newString)\n",
        "  newString = re.sub(r\"\\'m\", \" am\", newString)\n",
        "  \n",
        "  #remove words with numbers python: https://stackoverflow.com/a/18082370/4084039\n",
        "  newString = re.sub(\"\\S*\\d\\S*\", \"\", newString).strip()\n",
        "  \n",
        "  #remove html tags\n",
        "  newString = re.sub(r\"http\\S+\", \"\", newString)\n",
        "  \n",
        "  long_words=[]\n",
        "  for i in newString.split():\n",
        "    if len(i)>=3:                  \n",
        "      long_words.append(i)\n",
        "  return (\" \".join(long_words)).strip()\n",
        "\n",
        "\n",
        "data_new = text_cleaner(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EcEkMyu9ag9",
        "colab_type": "code",
        "outputId": "2f33d3bd-0b40-4333-ced2-ca012d252c0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data_new[:100]\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'when did beyonce start becoming popular the late what areas did beyonce compete when she was growing'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9p-Xc6VxkwU",
        "colab_type": "text"
      },
      "source": [
        "## Pre processing\n",
        "As part of pre-processing the cleaned text we will do the following steps before fitting the model.\n",
        "1. We will now create a list of character sequences of fixed length for the training of the language model.\n",
        "2. Encoding each character by means of assigning each character an unique number.\n",
        "3. Split the data into training set and validation set where X will be the first n-1 characters and Y will be the nth character."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Tpv_YQ69nlM",
        "colab_type": "code",
        "outputId": "5959ae2b-1157-4157-faac-eb2327e792c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def create_seq(text):\n",
        "  length = 30\n",
        "  sequences = list()\n",
        "  for i in range(length, len(text)):\n",
        "    seq = text[i-length:i+1]\n",
        "    sequences.append(seq)\n",
        "  print('Total Sequences: %d' % len(sequences))\n",
        "  return sequences\n",
        "\n",
        "sequences = create_seq(data_new)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Sequences: 8473433\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOeeYXdJ94ii",
        "colab_type": "code",
        "outputId": "f75e4dac-d7e2-47fc-c849-54f2f50b59d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "sequences[0:3]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['when did beyonce start becoming',\n",
              " 'hen did beyonce start becoming ',\n",
              " 'en did beyonce start becoming p']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--Dcou1-9_uB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Encode sequence\n",
        "set_chars = sorted(list(set(data_new)))\n",
        "mapping_dict = dict((c, i) for i, c in enumerate(set_chars))\n",
        "\n",
        "def encode_sequence(seqs):\n",
        "  encoded = list()\n",
        "  for seq in seqs:\n",
        "    encoded.append([mapping_dict[char] for char in seq])\n",
        "  return encoded\n",
        "\n",
        "sequences = encode_sequence(sequences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbDFzD950miB",
        "colab_type": "code",
        "outputId": "bbb53dfb-80f3-44fa-ccb2-5c76cbae1c5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# vocabulary size\n",
        "vocab_size = len(mapping_dict)\n",
        "sequences = np.array(sequences[:100000])\n",
        "# create X and y\n",
        "X, y = sequences[:,:-1], sequences[:,-1]\n",
        "# one hot encode y\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "# create train and validation sets\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.2, random_state=12)\n",
        "\n",
        "print('Train shape:', X_tr.shape, 'Val shape:', X_val.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train shape: (80000, 30) Val shape: (20000, 30)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1EwmaTlxsC8",
        "colab_type": "text"
      },
      "source": [
        "## Build the LSTM model\n",
        "Now we are ready to fir our LSTM model.\n",
        "* ***NOTEÂ : as part of building lstm model i have used two layered lstm networks and due to less computation resources I used very few data for the same. As part your activity you can play with the model by increasing or decreasing the layers, by changing the dropout rates, by changing the number of neurons in each layer, to get the best performance.***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaOkw36rCoIx",
        "colab_type": "code",
        "outputId": "0685b243-28c0-446e-991f-6eba048d5984",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 50, input_length=30, trainable=True))\n",
        "#model.add(GRU(150, recurrent_dropout=0.1, dropout=0.1))\n",
        "model.add(LSTM(128, activation='relu', recurrent_dropout=0.3, dropout=0.4, return_sequences=True))\n",
        "model.add(LSTM(128, activation='relu', recurrent_dropout=0.2, dropout=0.4))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "print(model.summary())\n",
        "\n",
        "# compile the model\n",
        "model.compile(loss='categorical_crossentropy', metrics=['acc'], optimizer='adam')\n",
        "# fit the model\n",
        "model.fit(X_tr, y_tr, epochs=50, verbose=2, validation_data=(X_val, y_val))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0814 12:21:11.492724 139823921956736 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0814 12:21:11.528764 139823921956736 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0814 12:21:11.534662 139823921956736 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0814 12:21:11.667651 139823921956736 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0814 12:21:11.678578 139823921956736 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0814 12:21:12.269939 139823921956736 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0814 12:21:12.293773 139823921956736 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0814 12:21:12.429786 139823921956736 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 30, 50)            1350      \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 30, 128)           91648     \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 27)                3483      \n",
            "=================================================================\n",
            "Total params: 228,065\n",
            "Trainable params: 228,065\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 80000 samples, validate on 20000 samples\n",
            "Epoch 1/50\n",
            " - 260s - loss: 2.5132 - acc: 0.2667 - val_loss: 2.0685 - val_acc: 0.3814\n",
            "Epoch 2/50\n",
            " - 255s - loss: 1.9950 - acc: 0.4043 - val_loss: 1.8020 - val_acc: 0.4599\n",
            "Epoch 3/50\n",
            " - 254s - loss: 1.8120 - acc: 0.4543 - val_loss: 1.6658 - val_acc: 0.4958\n",
            "Epoch 4/50\n",
            " - 254s - loss: 1.7078 - acc: 0.4856 - val_loss: 1.5875 - val_acc: 0.5181\n",
            "Epoch 5/50\n",
            " - 253s - loss: 1.6316 - acc: 0.5058 - val_loss: 1.5138 - val_acc: 0.5364\n",
            "Epoch 6/50\n",
            " - 254s - loss: 1.5826 - acc: 0.5180 - val_loss: 1.4826 - val_acc: 0.5487\n",
            "Epoch 7/50\n",
            " - 255s - loss: 1.5393 - acc: 0.5330 - val_loss: 1.4421 - val_acc: 0.5630\n",
            "Epoch 8/50\n",
            " - 256s - loss: 1.5052 - acc: 0.5413 - val_loss: 1.4219 - val_acc: 0.5678\n",
            "Epoch 9/50\n",
            " - 257s - loss: 1.4790 - acc: 0.5480 - val_loss: 1.4039 - val_acc: 0.5786\n",
            "Epoch 10/50\n",
            " - 256s - loss: 1.4562 - acc: 0.5544 - val_loss: 1.3826 - val_acc: 0.5827\n",
            "Epoch 11/50\n",
            " - 256s - loss: 1.4334 - acc: 0.5612 - val_loss: 1.3637 - val_acc: 0.5886\n",
            "Epoch 12/50\n",
            " - 256s - loss: 1.4167 - acc: 0.5647 - val_loss: 1.3510 - val_acc: 0.5947\n",
            "Epoch 13/50\n",
            " - 256s - loss: 1.4052 - acc: 0.5688 - val_loss: 1.3444 - val_acc: 0.5944\n",
            "Epoch 14/50\n",
            " - 255s - loss: 1.3904 - acc: 0.5748 - val_loss: 1.3283 - val_acc: 0.5980\n",
            "Epoch 15/50\n",
            " - 255s - loss: 1.3758 - acc: 0.5750 - val_loss: 1.3189 - val_acc: 0.6006\n",
            "Epoch 16/50\n",
            " - 255s - loss: 1.3646 - acc: 0.5803 - val_loss: 1.3179 - val_acc: 0.6027\n",
            "Epoch 17/50\n",
            " - 256s - loss: 1.3546 - acc: 0.5817 - val_loss: 1.3079 - val_acc: 0.6036\n",
            "Epoch 18/50\n",
            " - 256s - loss: 1.3455 - acc: 0.5847 - val_loss: 1.3067 - val_acc: 0.6077\n",
            "Epoch 19/50\n",
            " - 255s - loss: 1.3352 - acc: 0.5866 - val_loss: 1.2993 - val_acc: 0.6105\n",
            "Epoch 20/50\n",
            " - 255s - loss: 1.3289 - acc: 0.5885 - val_loss: 1.2962 - val_acc: 0.6119\n",
            "Epoch 21/50\n",
            " - 255s - loss: 1.3194 - acc: 0.5913 - val_loss: 1.2817 - val_acc: 0.6129\n",
            "Epoch 22/50\n",
            " - 255s - loss: 1.3101 - acc: 0.5938 - val_loss: 1.2815 - val_acc: 0.6121\n",
            "Epoch 23/50\n",
            " - 255s - loss: 1.3070 - acc: 0.5955 - val_loss: 1.2737 - val_acc: 0.6135\n",
            "Epoch 24/50\n",
            " - 255s - loss: 1.2971 - acc: 0.5970 - val_loss: 1.2770 - val_acc: 0.6179\n",
            "Epoch 25/50\n",
            " - 255s - loss: 1.2923 - acc: 0.5981 - val_loss: 1.2753 - val_acc: 0.6165\n",
            "Epoch 26/50\n",
            " - 255s - loss: 1.2858 - acc: 0.6016 - val_loss: 1.2660 - val_acc: 0.6202\n",
            "Epoch 27/50\n",
            " - 255s - loss: 1.2868 - acc: 0.5997 - val_loss: 1.2548 - val_acc: 0.6203\n",
            "Epoch 28/50\n",
            " - 255s - loss: 1.2766 - acc: 0.6038 - val_loss: 1.2655 - val_acc: 0.6183\n",
            "Epoch 29/50\n",
            " - 255s - loss: 1.2722 - acc: 0.6037 - val_loss: 1.2572 - val_acc: 0.6201\n",
            "Epoch 30/50\n",
            " - 254s - loss: 1.2674 - acc: 0.6045 - val_loss: 1.2560 - val_acc: 0.6198\n",
            "Epoch 31/50\n",
            " - 254s - loss: 1.2608 - acc: 0.6068 - val_loss: 1.2522 - val_acc: 0.6211\n",
            "Epoch 32/50\n",
            " - 254s - loss: 1.2593 - acc: 0.6054 - val_loss: 1.2498 - val_acc: 0.6246\n",
            "Epoch 33/50\n",
            " - 254s - loss: 1.2573 - acc: 0.6078 - val_loss: 1.2517 - val_acc: 0.6243\n",
            "Epoch 34/50\n",
            " - 255s - loss: 1.2526 - acc: 0.6094 - val_loss: 1.2465 - val_acc: 0.6276\n",
            "Epoch 35/50\n",
            " - 254s - loss: 1.2477 - acc: 0.6103 - val_loss: 1.2442 - val_acc: 0.6253\n",
            "Epoch 36/50\n",
            " - 254s - loss: 1.2435 - acc: 0.6124 - val_loss: 1.2370 - val_acc: 0.6288\n",
            "Epoch 37/50\n",
            " - 255s - loss: 1.2394 - acc: 0.6125 - val_loss: 1.2445 - val_acc: 0.6263\n",
            "Epoch 38/50\n",
            " - 255s - loss: 1.2345 - acc: 0.6122 - val_loss: 1.2420 - val_acc: 0.6288\n",
            "Epoch 39/50\n",
            " - 255s - loss: 1.2358 - acc: 0.6128 - val_loss: 1.2399 - val_acc: 0.6303\n",
            "Epoch 40/50\n",
            " - 255s - loss: 1.2259 - acc: 0.6168 - val_loss: 1.2372 - val_acc: 0.6317\n",
            "Epoch 41/50\n",
            " - 255s - loss: 1.2272 - acc: 0.6164 - val_loss: 1.2378 - val_acc: 0.6319\n",
            "Epoch 42/50\n",
            " - 255s - loss: 1.2218 - acc: 0.6179 - val_loss: 1.2405 - val_acc: 0.6287\n",
            "Epoch 43/50\n",
            " - 255s - loss: 1.2247 - acc: 0.6182 - val_loss: 1.2315 - val_acc: 0.6328\n",
            "Epoch 44/50\n",
            " - 255s - loss: 1.2217 - acc: 0.6168 - val_loss: 1.2318 - val_acc: 0.6313\n",
            "Epoch 45/50\n",
            " - 255s - loss: 1.2166 - acc: 0.6191 - val_loss: 1.2361 - val_acc: 0.6296\n",
            "Epoch 46/50\n",
            " - 255s - loss: 1.2145 - acc: 0.6187 - val_loss: 1.2319 - val_acc: 0.6300\n",
            "Epoch 47/50\n",
            " - 254s - loss: 1.2138 - acc: 0.6180 - val_loss: 1.2340 - val_acc: 0.6310\n",
            "Epoch 48/50\n",
            " - 254s - loss: 1.2060 - acc: 0.6223 - val_loss: 1.2316 - val_acc: 0.6352\n",
            "Epoch 49/50\n",
            " - 255s - loss: 1.2063 - acc: 0.6224 - val_loss: 1.2232 - val_acc: 0.6341\n",
            "Epoch 50/50\n",
            " - 255s - loss: 1.2027 - acc: 0.6223 - val_loss: 1.2285 - val_acc: 0.6372\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2af3440ba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SW1VD_lj6hw8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generate a sequence of characters with a language model\n",
        "def generate_seq(model, mapping_dict, seq_length, in_text, pred_chars):\n",
        "\tfor _ in range(pred_chars):\n",
        "\t\t# encode the characters\n",
        "\t\tencoded_seq = [mapping_dict[char] for char in in_text]\n",
        "\t\t\n",
        "    # truncate sequences to a fixed length if it's more\n",
        "\t\tencoded_seq = pad_sequences([encoded_seq], maxlen=seq_length, truncating='pre')\n",
        "\t\t\n",
        "    # predict character from the model\n",
        "\t\tyout = model.predict_classes(encoded_seq, verbose=0)\n",
        "\t\t\n",
        "\t\tout_char = ''\n",
        "\t\tfor char, index in mapping_dict.items():\n",
        "\t\t\tif index == yout:\n",
        "\t\t\t\tout_char = char\n",
        "\t\t\t\tbreak\n",
        "\t\t# append to input\n",
        "\t\tin_text += char\n",
        "\treturn in_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6UPMQzaBNZu",
        "colab_type": "code",
        "outputId": "cfc1605a-33fa-48bc-851d-172c66bfc113",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "generate_seq(model, mapping_dict, 30, 'mother', 15)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'mother what was the n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FC-0ySgOBPUe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "pick_file = data_dir + 'language_model1.pkl'\n",
        "with open(pick_file, 'wb') as f:\n",
        "  pickle.dump(model, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90ucJK9TBx6I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}